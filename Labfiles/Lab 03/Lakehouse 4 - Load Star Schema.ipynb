{"cells":[{"cell_type":"markdown","id":"17df4242-a0ba-47a2-a7d9-4a3cd1fa0d8a","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Lakehouse 4: Load Star Schema\n","This notebook will further process data from the hourly aggregation table into a dimension model.\n","Configure the variable **sourceTableName**, if needed, to match the hourly aggregation table."]},{"cell_type":"code","execution_count":null,"id":"de95bc6c-b751-4253-84f9-edb05e05496e","metadata":{},"outputs":[],"source":["from delta.tables import *\n","from pyspark.sql.functions import *\n","import datetime\n","from datetime import datetime\n","\n","sourceTableName = 'stocks_hour_agg'\n","\n","if not spark.catalog.tableExists(sourceTableName):\n","    msg = f'Error! Source table not found: {sourceTableName}'\n","    print(msg)\n","    raise SystemExit(msg)"]},{"cell_type":"code","execution_count":null,"id":"00abe306-aea6-4aa7-b27a-2f1d2e30fc7b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# this function adds symbols to dim_symbol that may not exist in table\n","# this allows for new symbols to be added to feed over time\n","\n","def dim_symbol_incremental_load(df_stocks, df_existing_symbols):\n","\n","    # determine max id of current symbols table\n","    if df_existing_symbols.rdd.isEmpty():\n","        maxId = 0\n","    else:\n","        maxId = df_existing_symbols.select(\"Symbol_SK\").rdd.max()[0]\n","\n","    # for the new rows to be ingested, get a list of unique symbols\n","    df_symbols = df_stocks.select(\"Symbol\").distinct().orderBy(\"Symbol\")\n","\n","    # get the symbols in the new dataset that do not exist in current symbols dimension\n","    df_symbols = df_symbols.join(df_existing_symbols, df_symbols.Symbol == df_existing_symbols.Symbol, \"left_outer\")\\\n","                        .where(df_existing_symbols.Symbol.isNull()) \\\n","                        .select(df_symbols.Symbol) \\\n","                        .orderBy(\"Symbol\")\n","\n","    df_symbols = df_symbols.withColumn(\"Symbol_SK\", monotonically_increasing_id() + maxId + 1)\n","    df_symbols = df_symbols.withColumn(\"Name\", when(df_symbols.Symbol == \"BCUZ\",\"Company Because\")\n","        .when(df_symbols.Symbol == \"IDGD\",\"Company IDontGiveADarn\")\n","        .when(df_symbols.Symbol == \"IDK\",\"Company IDontKnow\")\n","        .when(df_symbols.Symbol == \"TDY\",\"Company Today\")\n","        .when(df_symbols.Symbol == \"TMRW\",\"Company Tomorrow\")\n","        .when(df_symbols.Symbol == \"WHAT\",\"Company What\")\n","        .when(df_symbols.Symbol == \"WHY\",\"Company Why\")\n","        .when(df_symbols.Symbol == \"WHO\",\"Company Who\")\n","        .otherwise(\"Company Unknown\"))\n","    df_symbols = df_symbols.withColumn(\"Market\", when(substring(df_symbols.Symbol,1,1) == \"B\",\"NASDAQ\")\n","        .when(substring(df_symbols.Symbol,1,1) == \"W\",\"NASDAQ\")\n","        .when(substring(df_symbols.Symbol,1,1) == \"I\",\"NYSE\")\n","        .when(substring(df_symbols.Symbol,1,1) == \"T\",\"NYSE\")\n","        .otherwise(\"No Market\"))\n","    df_symbols = df_symbols.select(df_symbols.Symbol_SK, df_symbols.Symbol, df_symbols.Name, df_symbols.Market)\n","\n","    # if the dataframe is empty, there are no missing symbols\n","    if df_symbols.rdd.isEmpty():\n","        print(\"No new symbols.\") \n","        return df_existing_symbols\n","\n","    print(\"New Symbols:\")\n","    df_symbols.show()\n","\n","    dim_symbol_table = DeltaTable.forName(spark, \"dim_symbol\")\n","\n","    dim_symbol_table.alias('dim_symbol') \\\n","    .merge(\n","        df_symbols.alias('updates'),\n","        'dim_symbol.Symbol = updates.Symbol'\n","    ) \\\n","    .whenNotMatchedInsert(values =\n","        {\n","            \"Symbol_SK\": \"updates.Symbol_SK\"\n","            ,\"Symbol\": \"updates.Symbol\"\n","            ,\"Name\": \"updates.Name\"\n","            ,\"Market\": \"updates.Market\"\n","        }\n","    ) \\\n","    .execute()\n","\n","    return spark.sql(\"SELECT * FROM dim_symbol ORDER BY Symbol ASC\")"]},{"cell_type":"code","execution_count":null,"id":"42e48b7e-8a84-4a15-a61d-65111639c7d3","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# find latest date \n","\n","df_watermark = spark.sql(f\"SELECT PriceDateKey \\\n","    FROM fact_stocks_daily_prices \\\n","    ORDER BY PriceDateKey DESC LIMIT 1\")\n","\n","if not df_watermark.rdd.isEmpty():\n","    df_watermark.show()\n","    maxDate = df_watermark.first()[\"PriceDateKey\"]\n","    cutoff_datetime = datetime(maxDate.year, maxDate.month, maxDate.day, 0, 0, 0)\n","else:\n","    print('Table is empty, using default date.')\n","    cutoff_datetime = '2000-01-01 00:00:00'\n","\n","# manually specify a cutoff date\n","#cutoff_datetime = '2023-11-27 23:59:50'\n","\n","print(f\"Cutoff date: {cutoff_datetime}\")"]},{"cell_type":"code","execution_count":null,"id":"ee0db926-e978-453e-858e-209b26c0da94","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# get new stock data to ingest, starting at watermark\n","# limit is arbitrary; limited primarily for demo purposes\n","\n","df_stocks = spark.sql(f\"SELECT Symbol, MinPrice, MaxPrice, LastPrice, Datestamp, Hour FROM {sourceTableName} \\\n","    WHERE Datestamp >= '{cutoff_datetime}' \\\n","    ORDER BY Datestamp ASC, Hour ASC LIMIT 5000000\")\n","df_stocks.show()"]},{"cell_type":"code","execution_count":null,"id":"c580fedb-bbd1-484a-aeac-f1549da68006","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# load the date dimension for later joins\n","\n","df_date = spark.sql(\"SELECT * FROM dim_date\")\n","df_date.show()"]},{"cell_type":"code","execution_count":null,"id":"e9f3cc02-c94a-4825-9761-dffc96d34434","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# load the symbols dimension \n","\n","# creating the symbols incremental load in this way allows new symbols\n","# to be added over time dynamically. if new symbols are found in the \n","# new stock data, they will be imported into the symbol dimension\n","# before continuing\n","\n","df_symbol = spark.sql(\"SELECT * FROM dim_symbol ORDER BY Symbol ASC\")\n","print(\"Current Symbols:\")\n","df_symbol.show()\n","\n","# load any new symbols into dimension\n","df_symbol = dim_symbol_incremental_load(df_stocks, df_symbol)\n","\n","print(\"Symbols After Merge:\")\n","df_symbol.show()"]},{"cell_type":"code","execution_count":null,"id":"eebc1709-cc23-4c90-970a-2ce78befa23d","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Code generated by Data Wrangler for PySpark DataFrame\n","\n","from pyspark.sql import functions as F\n","\n","def clean_data(df_stocks):\n","    # df_stocks = df_stocks.withColumn('datestamp', to_date(df_stocks['timestamp']))\n","    df_stocks = df_stocks.groupBy('Symbol', 'Datestamp').agg(F.min('MinPrice').alias('newMinPrice'), \n","        F.max('MAxPrice').alias('newMaxPrice'), F.last('LastPrice').alias('newClosePrice'))\n","    df_stocks = df_stocks.dropna()\n","    df_stocks = df_stocks.sort(df_stocks['Symbol'].asc(), df_stocks['Datestamp'].asc())\n","    return df_stocks\n","\n","df_stocks_agg = clean_data(df_stocks)\n","display(df_stocks_agg)"]},{"cell_type":"code","execution_count":null,"id":"fb70e7dd-6b5a-4c44-a857-dca3c65292e4","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# join the aggregated data to the date dimension\n","\n","df_join = df_stocks_agg.join(df_date, df_stocks_agg.Datestamp == df_date.DateKey)\n","display(df_join)"]},{"cell_type":"code","execution_count":null,"id":"0b0702c7-5309-4001-b1b4-c05a4df5fea3","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# join the data from above with the symbols dimension\n","\n","df_join = df_join.join(df_symbol, df_join.Symbol == df_symbol.Symbol)\n","display(df_join)"]},{"cell_type":"code","execution_count":null,"id":"81c47252-f6f4-4aac-972c-c3e13ae50593","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# create a final view with cleaned names for processing ease\n","\n","df_final_view = df_join.select(col(\"DateKey\").alias(\"newPriceDateKey\"), col(\"dim_symbol.Symbol\").alias(\"newSymbol\"),\n","    col(\"dim_symbol.Symbol_SK\").alias(\"newSymbol_SK\"),\"newMinPrice\",\"newMaxPrice\",\"newClosePrice\")\n","\n","df_final_view.show()"]},{"cell_type":"code","execution_count":null,"id":"b370e070-f9c2-47e9-a7e3-d8be4090eda0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# to insert the new data, we'll merge the dataframe with the fact table.\n","# for existing records, update the high/low/close price of the stock\n","# for new records, insert a new row with the current high/low/close\n","\n","from delta.tables import *\n","\n","fact_stock_prices_table = DeltaTable.forName(spark, \"fact_stocks_daily_prices\")\n","\n","fact_stock_prices_table.alias('fact') \\\n","  .merge(\n","    df_final_view.alias('updates'),\n","    'fact.PriceDateKey = updates.newPriceDateKey and fact.Symbol_SK = updates.newSymbol_SK'\n","  ) \\\n","  .whenMatchedUpdate(set =\n","    {\n","        \"MinPrice\": \"CASE WHEN fact.MinPrice < updates.newMinPrice THEN fact.MinPrice ELSE updates.newMinPrice END\"\n","        ,\"MaxPrice\": \"CASE WHEN fact.MaxPrice > updates.newMaxPrice THEN fact.MaxPrice ELSE updates.newMaxPrice END\"\n","        ,\"ClosePrice\": \"updates.newClosePrice\"\n","    }\n","  ) \\\n","  .whenNotMatchedInsert(values =\n","    {\n","        \"Symbol_SK\": \"updates.newSymbol_SK\"\n","        ,\"PriceDateKey\": \"updates.newPriceDateKey\"\n","        ,\"MinPrice\": \"updates.newMinPrice\"\n","        ,\"MaxPrice\": \"updates.newMaxPrice\"\n","        ,\"ClosePrice\": \"updates.newClosePrice\"\n","    }\n","  ) \\\n","  .execute()\n"]},{"cell_type":"markdown","id":"da564b67-1651-475b-a735-5591b42079fc","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["The code below is for observing the output and comparing results."]},{"cell_type":"code","execution_count":null,"id":"846e91e1-09eb-46a0-ad5c-041c95bdb5c3","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# function that gets the latest fact data\n","\n","def get_latest_fact():\n","    return spark.sql(\"SELECT dim.Symbol, fact.Symbol_SK, PriceDateKey, MinPrice, MaxPrice, ClosePrice \\\n","        FROM fact_stocks_daily_prices fact \\\n","        INNER JOIN dim_symbol dim on fact.Symbol_SK = dim.Symbol_SK \\\n","        WHERE PriceDateKey >= date_add(current_date(),-30) \\\n","        ORDER BY PriceDateKey ASC, fact.Symbol_SK ASC\")"]},{"cell_type":"code","execution_count":null,"id":"67c24202-b337-4af8-8d55-550a5dcf5d0e","metadata":{"collapsed":false,"editable":true,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[],"source":["# run results:\n","df_run = get_latest_fact()\n","display(df_run)"]},{"cell_type":"code","execution_count":null,"id":"d1f70087","metadata":{},"outputs":[],"source":["import plotly.express as px\n","import plotly.graph_objects as go\n","import pandas as pd\n","\n","df_run_pd = df_run.toPandas()\n","symbols_pd = sorted(df_run_pd['Symbol'].unique())\n","\n","fig = go.Figure()\n","\n","for symbol in symbols_pd:\n","    dftemp = df_run_pd.loc[df_run_pd['Symbol'] == symbol][[\"PriceDateKey\",\"ClosePrice\"]]\n","    fig.add_trace(go.Scatter(x=dftemp['PriceDateKey'], y=dftemp['ClosePrice'], name=symbol, line=dict(width=1)))\n","\n","fig.update_layout(title=\"Close Price - Last 30 Days\", showlegend=True)\n","fig.show()"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
