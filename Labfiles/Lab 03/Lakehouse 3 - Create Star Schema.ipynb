{"cells":[{"cell_type":"markdown","id":"071a8446-0a66-4beb-a194-d72157f16aa6","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Lakehouse 3 - Create Star Schema\n","This run-once notebook will setup the schema for building the fact and dimension tables.\n","Configure the sourceTableName variable in the first cell (if needed) to match the hourly aggregation table. The begin/end dates are for the date dimension table. This notebook will recreate all tables, rebuilding the schema: existing fact and dimension tables will be overwritten."]},{"cell_type":"code","execution_count":null,"id":"9bd16c06-c5b9-41f8-92f2-e482401886ba","metadata":{},"outputs":[],"source":["# configure the source table name (if needed) and begin/end dates for the date dimension\n","\n","from delta.tables import *\n","from pyspark.sql.functions import *\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import concat, col, lit, when, substring, monotonically_increasing_id \n","from datetime import datetime\n","\n","# name of source table -- default is the hourly aggregation table\n","sourceTableName = 'stocks_hour_agg'\n","\n","# begin/end dates for dim_date table\n","beginYear = 2023\n","endYear = datetime.today().year + 2\n","\n","if not spark.catalog.tableExists(sourceTableName):\n","    msg = f'Warning! Source table not found: {sourceTableName}'\n","    print(msg)"]},{"cell_type":"code","execution_count":null,"id":"112a629f-33dd-4f02-bb5c-5d4828e0c5d9","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# main fact table for stock data\n","\n","def create_fact_Stocks_Daily_Prices():\n","    spark.sql(f\"\"\"\n","        CREATE OR REPLACE TABLE fact_Stocks_Daily_Prices (\n","            Symbol_SK LONG NOT NULL\n","            ,PriceDateKey DATE \n","            ,MinPrice DOUBLE \n","            ,MaxPrice DOUBLE \n","            ,ClosePrice DOUBLE)\n","        USING DELTA\n","        \"\"\")\n","    \n","create_fact_Stocks_Daily_Prices()"]},{"cell_type":"code","execution_count":null,"id":"00869ec8-98cf-4fbf-9972-bbc30eb6aee3","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# symbol dimension holds details about each company\n","\n","def create_dim_symbol():\n","    spark.sql(f\"\"\"\n","        CREATE OR REPLACE TABLE dim_symbol (\n","            Symbol_SK LONG NOT NULL\n","            ,Symbol VARCHAR(5) NOT NULL\n","            ,Name VARCHAR(25)\n","            ,Market VARCHAR(15) )\n","        USING DELTA\n","        \"\"\")\n","\n","create_dim_symbol()"]},{"cell_type":"code","execution_count":null,"id":"e6ce3035-2a2d-448e-adcf-7de1696ee50e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# load the symbol table by getting a list of distinct symbols from source table\n","\n","def dim_symbol_initial_load(sourceTableName):\n","\n","    if not spark.catalog.tableExists(sourceTableName):\n","        msg = f'Warning! Source table not found: {sourceTableName}. Will not load stock symbols.'\n","        print(msg)\n","        return\n","\n","    # get unique stock symbols in source table\n","    df_stocks = spark.sql(f\"SELECT distinct(Symbol), DENSE_RANK() OVER(ORDER BY Symbol asc) row FROM {sourceTableName}\")\n","\n","    df_symbols = df_stocks.select(\"Symbol\", \"row\")\n","    df_symbols = df_symbols.withColumn(\"Symbol_SK\", col(\"row\"))\n","    df_symbols = df_symbols.withColumn(\"Name\", when(df_symbols.Symbol == \"BCUZ\",\"Company Because\") \\\n","        .when(df_symbols.Symbol == \"IDGD\",\"Company IDontGiveADarn\") \\\n","        .when(df_symbols.Symbol == \"IDK\",\"Company IDontKnow\") \\\n","        .when(df_symbols.Symbol == \"TDY\",\"Company Today\") \\\n","        .when(df_symbols.Symbol == \"TMRW\",\"Company Tomorrow\") \\\n","        .when(df_symbols.Symbol == \"WHAT\",\"Company What\") \\\n","        .when(df_symbols.Symbol == \"WHY\",\"Company Why\") \\\n","        .when(df_symbols.Symbol == \"WHO\",\"Company Who\") \\\n","        .otherwise(\"Company Unknown\"))\n","    df_symbols = df_symbols.withColumn(\"Market\", when(substring(df_symbols.Symbol,1,1) == \"B\",\"NASDAQ\") \\\n","                            .when(substring(df_symbols.Symbol,1,1) == \"W\",\"NASDAQ\") \\\n","                            .when(substring(df_symbols.Symbol,1,1) == \"I\",\"NYSE\") \\\n","                            .when(substring(df_symbols.Symbol,1,1) == \"T\",\"NYSE\") \\\n","                            .otherwise(\"No Market\"))\n","\n","    # merge the symbols into the table\n","    dim_symbol_table = DeltaTable.forName(spark, \"dim_symbol\")\n","\n","    dim_symbol_table.alias('dim_symbol') \\\n","    .merge( \\\n","        df_symbols.alias('updates'), \\\n","        'dim_symbol.Symbol = updates.Symbol' \\\n","    ) \\\n","    .whenNotMatchedInsert(values = \\\n","        { \n","            \"Symbol_SK\": \"updates.Symbol_SK\"\n","            ,\"Symbol\": \"updates.Symbol\"\n","            ,\"Name\": \"updates.Name\"\n","            ,\"Market\": \"updates.Market\"\n","        } \\\n","    ) \\\n","    .execute()\n","\n","    df_dimSymbol = spark.sql(\"SELECT * FROM dim_symbol\")\n","    df_dimSymbol.show()\n","\n","dim_symbol_initial_load(sourceTableName)"]},{"cell_type":"code","execution_count":null,"id":"849af595-b53e-44a3-b15d-ef77c1cec60d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# create and populate the date dimension\n","\n","def createAndPopulate_dim_date(beginYear=2022, endYear=2025):\n","\n","    # Create a DataFrame with a range of dates\n","    dates = spark.range(\n","        (datetime(endYear, 12, 31) - datetime(beginYear, 1, 1)).days + 1\n","    ).select(\n","        (date_add(lit(f\"{beginYear}-01-01\"), col(\"id\").cast(\"int\"))).alias(\"date\")\n","    )\n","\n","    # Select the desired columns\n","    datesdf = dates.select(\n","        date_format(\"date\",\"yyyy-MM-dd\").cast('date').alias(\"DateKey\"),\n","        dayofmonth(\"date\").alias(\"DayNum\"),\n","        dayofweek(\"date\").alias(\"DayOfWeekNum\"),\n","        date_format(\"date\", \"EEEE\").alias(\"DayOfWeekName\"),\n","        month(\"date\").alias(\"MonthNum\"),\n","        date_format(\"date\", \"MMMM\").alias(\"MonthName\"),\n","        quarter(\"date\").alias(\"QuarterNum\"),\n","        concat(lit(\"Q\"), quarter(\"date\")).alias(\"QuarterName\"),\n","        year(\"date\").alias(\"Year\")\n","    )\n","\n","    datesdf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"dim_date\")\n","    datesdf.show()\n","\n","createAndPopulate_dim_date(beginYear, endYear)"]},{"cell_type":"code","execution_count":null,"id":"0bddc9fc-3946-4b63-a189-075836e6b31e","metadata":{"editable":true,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[],"source":["# drop tables for testing\n","\n","def dropTables():\n","    spark.sql(\"DROP TABLE fact_stocks_daily_prices\")\n","    spark.sql(\"DROP TABLE dim_symbol\")\n","    spark.sql(\"DROP TABLE dim_date\")\n","\n","# dropTables()"]},{"cell_type":"code","execution_count":null,"id":"55f55603-639f-4e2b-abe9-cc6633eff1f9","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# optional optimization, recommended for small tables\n","# typically scheduled for regular maintenance\n","\n","def optimizeTables(sourceTableName):\n","\n","    dim_date_table = DeltaTable.forName(spark, \"dim_date\")\n","    dim_date_table.optimize().executeCompaction()\n","\n","    dim_symbol_table = DeltaTable.forName(spark, \"dim_symbol\")\n","    dim_symbol_table.optimize().executeCompaction()\n","\n","    fact_stock_prices_table = DeltaTable.forName(spark, \"fact_stocks_daily_prices\")\n","    fact_stock_prices_table.optimize().executeCompaction()\n","\n","    if spark.catalog.tableExists(sourceTableName):\n","        StockData_table = DeltaTable.forName(spark, sourceTableName)\n","        StockData_table.optimize().executeCompaction()\n","\n","optimizeTables(sourceTableName)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
