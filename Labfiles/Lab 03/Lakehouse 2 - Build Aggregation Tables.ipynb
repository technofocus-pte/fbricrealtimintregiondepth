{"cells":[{"cell_type":"markdown","id":"6b2b0d68-c78e-47be-a86b-11ba014e2e71","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Lakehouse 2 - Build Aggregate Tables\n","\n","In this notebook, we'll build data aggregation tables to be used for reporting, data science, and other processes. \n","\n","Data Wrangler will be used to accomplish this task, which can also be used to cleanse the data to remove invalid data. The output will be two tables that summarize the data: *stocks_minute_agg* and *stocks_hour_agg* tables. The minute table aggregates the incoming stock feed to the minute, while the hour table summarizes the table to the hour.\n","\n","**NOTE:** this notebook is designed to be used interactively, using data wrangler for cleansing and aggregation. Completed data wrangler steps are commented out below that can be used as reference. See the lab guide for more information. Once the notebook is completed, it can be scheduled to run periodically."]},{"cell_type":"code","execution_count":null,"id":"e7c5fba8-281f-4600-a6ba-3b68ad7ff7a4","metadata":{},"outputs":[],"source":["from delta.tables import *\n","from pyspark.sql.functions import *\n","import datetime\n","import time\n","from datetime import datetime\n","from datetime import timedelta\n","\n","sourceTableName = 'raw_stock_data'"]},{"cell_type":"code","execution_count":null,"id":"ad739935-6a30-4dbc-acab-2292f68a2fa4","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def create_minute_aggregation_table():\n","    spark.sql(f\"\"\"\n","        CREATE TABLE IF NOT EXISTS stocks_minute_agg (\n","            Symbol VARCHAR(5) NOT NULL\n","            ,Datestamp DATE \n","            ,Hour INT\n","            ,Minute INT\n","            ,MinPrice DOUBLE \n","            ,MaxPrice DOUBLE \n","            ,LastPrice DOUBLE\n","            )\n","        USING DELTA\n","        \"\"\")\n","\n","create_minute_aggregation_table()"]},{"cell_type":"code","execution_count":null,"id":"309e8c91-4eb6-4040-95ce-beb8bc17578b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["def create_hour_aggregation_table():\n","    spark.sql(f\"\"\"\n","        CREATE TABLE IF NOT EXISTS stocks_hour_agg (\n","            Symbol VARCHAR(5) NOT NULL\n","            ,Datestamp DATE \n","            ,Hour INT\n","            ,MinPrice DOUBLE \n","            ,MaxPrice DOUBLE \n","            ,LastPrice DOUBLE\n","            )\n","        USING DELTA\n","        \"\"\")\n","\n","create_hour_aggregation_table()"]},{"cell_type":"code","execution_count":null,"id":"63ab6b60-434f-49ce-a3af-a19359625220","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from pyspark.sql import Row\n","\n","def createAnomalyDataframe():\n","\n","    df = spark.createDataFrame([\n","        Row(symbol=\"WHO\", price=250.55, timestamp='2023-10-01 04:40:00')\n","        ,Row(symbol=\"WHAT\", price=432.23, timestamp='2023-10-01 04:40:00')\n","        ,Row(symbol=None, price=124.33, timestamp='2023-10-06 00:00:00')\n","        ,Row(symbol=\"WHY\", price=103.11, timestamp='2023-10-01 04:40:00')\n","        ,Row(symbol=\"BCUZ\", price=12.0, timestamp='2023-10-02 00:00:00')\n","        ,Row(symbol=\"IDK\", price=0.0, timestamp='2023-10-04 17:00:00')\n","        ,Row(symbol=\"IDGD\", price=0.0, timestamp='2023-10-04 17:00:00')\n","        ,Row(symbol=\"TMRW\", price=None, timestamp='2023-10-06 00:00:00')\n","        ,Row(symbol=\"TDY\", price=None, timestamp='2023-10-06 00:00:00')\n","    ])\n","\n","    return df\n","\n","# anomaly data frame is used so we can visualize the process of simple data cleansing\n","anomaly_df = createAnomalyDataframe()"]},{"cell_type":"code","execution_count":null,"id":"34183e9b-036e-41ff-a5fd-d4781bd8993f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# to insert the new data, we'll merge the dataframe with the fact table\n","# for existing records, update the high/low/close price of the stock\n","# for new records, insert a new row with the current high/low/close\n","\n","from delta.tables import *\n","\n","def merge_minute_agg(df):\n","    stocks_minute_agg_table = DeltaTable.forName(spark, \"stocks_minute_agg\")\n","\n","    stocks_minute_agg_table.alias('table') \\\n","    .merge(\n","        df.alias('updates'),\n","        'table.symbol = updates.symbol and table.datestamp = updates.datestamp and table.hour = updates.hour and table.minute = updates.minute'\n","    ) \\\n","    .whenMatchedUpdate(set =\n","        {\n","            \"MinPrice\": \"CASE WHEN table.MinPrice < updates.price_min THEN table.MinPrice ELSE updates.price_min END\"\n","            ,\"MaxPrice\": \"CASE WHEN table.MaxPrice > updates.price_max THEN table.MaxPrice ELSE updates.price_max END\"\n","            ,\"LastPrice\": \"updates.price_last\"\n","        }\n","    ) \\\n","    .whenNotMatchedInsert(values =\n","        {\n","            \"Symbol\": \"updates.symbol\"\n","            ,\"Datestamp\": \"updates.datestamp\"\n","            ,\"Hour\": \"updates.hour\"\n","            ,\"Minute\": \"updates.minute\"\n","            ,\"MinPrice\": \"updates.price_min\"\n","            ,\"MaxPrice\": \"updates.price_max\"\n","            ,\"LastPrice\": \"updates.price_last\"\n","        }\n","    ) \\\n","    .execute()\n"]},{"cell_type":"code","execution_count":null,"id":"774bc79c-5a72-4ec2-84f6-306db3bbe69c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# to insert the new data, we'll merge the dataframe with the fact table\n","# for existing records, update the high/low/close price of the stock\n","# for new records, insert a new row with the current high/low/close\n","\n","from delta.tables import *\n","\n","def merge_hour_agg(df):\n","    stocks_minute_agg_table = DeltaTable.forName(spark, \"stocks_hour_agg\")\n","\n","    stocks_minute_agg_table.alias('table') \\\n","    .merge(\n","        df.alias('updates'),\n","        'table.symbol = updates.symbol and table.datestamp = updates.datestamp and table.hour = updates.hour'\n","    ) \\\n","    .whenMatchedUpdate(set =\n","        {\n","            \"MinPrice\": \"CASE WHEN table.MinPrice < updates.price_min THEN table.MinPrice ELSE updates.price_min END\"\n","            ,\"MaxPrice\": \"CASE WHEN table.MaxPrice > updates.price_max THEN table.MaxPrice ELSE updates.price_max END\"\n","            ,\"LastPrice\": \"updates.price_last\"\n","        }\n","    ) \\\n","    .whenNotMatchedInsert(values =\n","        {\n","            \"Symbol\": \"updates.symbol\"\n","            ,\"Datestamp\": \"updates.datestamp\"\n","            ,\"Hour\": \"updates.hour\"\n","            ,\"MinPrice\": \"updates.price_min\"\n","            ,\"MaxPrice\": \"updates.price_max\"\n","            ,\"LastPrice\": \"updates.price_last\"\n","        }\n","    ) \\\n","    .execute()\n"]},{"cell_type":"code","execution_count":null,"id":"e3f22493-6119-4fec-b677-087568e6402f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# find latest date \n","\n","df_watermark = spark.sql(f\"SELECT datestamp, hour, minute \\\n","    FROM stocks_minute_agg \\\n","    ORDER BY Datestamp DESC, Hour DESC, Minute DESC LIMIT 1\")\n","\n","if not df_watermark.rdd.isEmpty():\n","    df_watermark.show()\n","    maxDate = df_watermark.first()[\"datestamp\"]\n","    maxHour = df_watermark.first()[\"hour\"]\n","    maxMinute = df_watermark.first()[\"minute\"]\n","    cutoff_datetime = datetime(maxDate.year, maxDate.month, maxDate.day, maxHour, maxMinute, 0)\n","else:\n","    print('Table is empty, using default date.')\n","    cutoff_datetime = '2000-01-01 00:00:00'\n","\n","# manually specify a cutoff date\n","#cutoff_datetime = '2023-11-27 23:59:50'\n","\n","print(f\"Cutoff date: {cutoff_datetime}\")\n"]},{"cell_type":"code","execution_count":null,"id":"c46e2020-d469-4c2a-a4ac-94b07b4159f0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# get new stock data to ingest, starting at watermark\n","# limit is arbitrary; limited primarily for demo purposes\n","\n","df_stocks = spark.sql(f\"SELECT symbol, price, timestamp FROM {sourceTableName} \\\n","    WHERE timestamp >= '{cutoff_datetime}' \\\n","    ORDER BY timestamp ASC LIMIT 5000000\")\n","df_stocks.show()\n"]},{"cell_type":"markdown","id":"f273a1ba-79a9-4c0c-a172-2a7cbc2881f1","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["#### Add data wrangler code here to:\n","1. Drop missing values from symbol and price\n","2. Drop values from price where price = 0\n","\n","Name the resulting dataframe: df_stocks_clean. See commented-out sample for reference of output.\n","\n","Pro tip: CTRL / (Control + forward slash) will comment/uncomment selected text"]},{"cell_type":"code","execution_count":null,"id":"9d8a72da-b6e7-4e0b-be3d-4f533fa844c6","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Add data wrangler here. Select this cell and click Data > Transform Data with Data Wrangler\n","# If you get stuck, refer to the example commented out below\n","\n","# Name the resulting dataframe: df_stocks_clean"]},{"cell_type":"code","execution_count":null,"id":"da5d3ae6-6bf6-4b35-9a51-33751c24cfcc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# # Code generated by Data Wrangler for PySpark DataFrame\n","\n","# def remove_invalid_rows(df):\n","#     # Drop rows with missing data in columns: 'symbol', 'price'\n","#     df = df.dropna(subset=['symbol', 'price'])\n","#     # Filter rows based on column: 'price'\n","#     df = df.filter(~(df['price'] == 0))\n","#     return df\n","\n","# df_stocks_clean = remove_invalid_rows(df_stocks)\n","# display(df_stocks_clean)"]},{"cell_type":"markdown","id":"d56dfba3-4cae-4090-ac65-143f635be837","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["#### Symbol/Date/Hour/Minute Aggregation Section\n","\n","Add data wrangler code here to:\n","1. Add new column datestamp, based on timestamp (date without a time component)\n","2. Add new columm hour, based on timestamp hour value\n","3. Add new column minute, based on timestamp minute value\n","4. Convert column hour to an integer type (int32)\n","5. Convert column minute to an integer type (int32)\n","6. Group By symbol, datestamp, hour, and minute\n","    1. Add aggregations for minimum price, maximum price, and last price\n","\n","Name the resulting dataframe: df_stocks_agg_minute. See commented-out sample for reference of output.\n","\n","Pro tip: CTRL / (Control + forward slash) will comment/uncomment selected text"]},{"cell_type":"code","execution_count":null,"id":"cb42546b-6f83-4621-8d6b-1c39aa98c5a7","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Add data wrangler here. Select this cell and click Data > Transform Data with Data Wrangler\n","# If you get stuck, refer to the example commented out below\n","\n","# Name the resulting dataframe: df_stocks_agg_minute"]},{"cell_type":"code","execution_count":null,"id":"e7f57a45-99d0-4697-a25e-1a3f06426dea","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# # Code generated by Data Wrangler for PySpark DataFrame\n","\n","# from datetime import datetime\n","# from pyspark.sql import functions as F\n","# from pyspark.sql import types as T\n","\n","# def aggregate_data_minute(df_stocks_clean):\n","#     # Derive column 'datestamp' from column: 'timestamp'\n","    \n","#     # Transform based on the following examples:\n","#     #    timestamp           Output\n","#     # 1: 2023-11-28T02:00 => \"2023-11-28\"\n","#     udf_fn = F.udf(lambda v : v.strftime(\"%Y-%m-%d\"), T.StringType())\n","#     df_stocks_clean = df_stocks_clean.withColumn(\"datestamp\", udf_fn(F.col(\"timestamp\")))\n","#     # Derive column 'hour' from column: 'timestamp'\n","    \n","#     def hour(timestamp):\n","#         \"\"\"\n","#         Transform based on the following examples:\n","#            timestamp           Output\n","#         1: 2023-11-28T02:00 => \"2\"\n","#         2: 2023-11-28T03:00 => \"3\"\n","#         \"\"\"\n","#         number1 = timestamp.hour\n","#         return f\"{number1:01.0f}\"\n","    \n","#     udf_fn = F.udf(lambda v : hour(v), T.StringType())\n","#     df_stocks_clean = df_stocks_clean.withColumn(\"hour\", udf_fn(F.col(\"timestamp\")))\n","#     # Derive column 'minute' from column: 'timestamp'\n","    \n","#     def minute(timestamp):\n","#         \"\"\"\n","#         Transform based on the following examples:\n","#            timestamp           Output\n","#         1: 2023-11-28T02:01 => \"1\"\n","#         2: 2023-11-28T02:02 => \"2\"\n","#         \"\"\"\n","#         number1 = timestamp.minute\n","#         return f\"{number1:01.0f}\"\n","    \n","#     udf_fn = F.udf(lambda v : minute(v), T.StringType())\n","#     df_stocks_clean = df_stocks_clean.withColumn(\"minute\", udf_fn(F.col(\"timestamp\")))\n","#     # Change column type to int32 for column: 'hour'\n","#     df_stocks_clean = df_stocks_clean.withColumn('hour', df_stocks_clean['hour'].cast(T.IntegerType()))\n","#     # Change column type to int32 for column: 'minute'\n","#     df_stocks_clean = df_stocks_clean.withColumn('minute', df_stocks_clean['minute'].cast(T.IntegerType()))\n","#     # Performed 3 aggregations grouped on columns: 'symbol', 'datestamp' and 2 other columns\n","#     df_stocks_clean = df_stocks_clean.groupBy('symbol', 'datestamp', 'hour', 'minute').agg(F.max('price').alias('price_max'), F.min('price').alias('price_min'), F.last('price').alias('price_last'))\n","#     df_stocks_clean = df_stocks_clean.dropna()\n","#     df_stocks_clean = df_stocks_clean.sort(df_stocks_clean['symbol'].asc(), df_stocks_clean['datestamp'].asc(), df_stocks_clean['hour'].asc(), df_stocks_clean['minute'].asc())\n","#     return df_stocks_clean\n","\n","# df_stocks_agg_minute = aggregate_data_minute(df_stocks_clean)\n","# display(df_stocks_agg_minute)"]},{"cell_type":"code","execution_count":null,"id":"f9cc8449-20b9-4348-b8b4-e264738256c4","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# write the data to the stocks_minute_agg table\n","\n","merge_minute_agg(df_stocks_agg_minute)"]},{"cell_type":"markdown","id":"765c9107-781b-43a4-a94d-03237e7b8823","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["#### Symbol/Date/Hour Aggregation Section\n","\n","Add data wrangler code here to:\n","1. Group By symbol, datestamp, hour\n","    1. Add aggregations for minimum price, maximum price, and last price\n","\n","In the generated code, modify the aliases to keep price_max, price_min, price_last to the same name. Because we're aggregating the data a second time, the default naming will try to set the names to price_max_max, price_min_min, etc.\n","\n","Name the resulting dataframe: df_stocks_agg_hour. See commented-out sample for reference of output.\n","\n","Pro tip: CTRL / (Control + forward slash) will comment/uncomment selected text"]},{"cell_type":"code","execution_count":null,"id":"f1dd493b-027d-4b36-abf5-30c6836d982c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Add data wrangler here. Select this cell and click Data > Transform Data with Data Wrangler\n","# If you get stuck, refer to the example commented out below\n","\n","# Name the resulting dataframe: df_stocks_agg_hour"]},{"cell_type":"code","execution_count":null,"id":"8735c0ce-8ccc-4440-88c7-7320e51e0886","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# # Code generated by Data Wrangler for PySpark DataFrame\n","\n","# from pyspark.sql import functions as F\n","\n","# def aggregate_data_hour(df_stocks_agg_minute):\n","#     # Performed 3 aggregations grouped on columns: 'symbol', 'datestamp', 'hour'\n","#     df_stocks_agg_minute = df_stocks_agg_minute.groupBy('symbol', 'datestamp', 'hour').agg(\n","#         F.max('price_max').alias('price_max'), \n","#         F.min('price_min').alias('price_min'), \n","#         F.last('price_last').alias('price_last'))\n","#     df_stocks_agg_minute = df_stocks_agg_minute.dropna()\n","#     df_stocks_agg_minute = df_stocks_agg_minute.sort(df_stocks_agg_minute['symbol'].asc(), df_stocks_agg_minute['datestamp'].asc(), df_stocks_agg_minute['hour'].asc())\n","#     return df_stocks_agg_minute\n","\n","# df_stocks_agg_hour = aggregate_data_hour(df_stocks_agg_minute)\n","# display(df_stocks_agg_hour)"]},{"cell_type":"code","execution_count":null,"id":"2f22c691-33c1-45e4-a4eb-74d50d9b4029","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# write the data to the stocks_hour_agg table\n","\n","merge_hour_agg(df_stocks_agg_hour)"]},{"cell_type":"markdown","id":"b04627ca-828c-4726-83c3-e73bc9b8f2f7","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["#### Handy SQL Commands for testing\n","Use this section for testing, cleaning out tables to re-run, etc."]},{"cell_type":"code","execution_count":null,"id":"a52f73cd-c685-481c-a0b4-7d624ab18437","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df = spark.sql(\"SELECT min(Datestamp) as mindate, max(Datestamp) as maxdate FROM stocks_minute_agg\")\n","df.show()\n","\n","df = spark.sql(\"SELECT * FROM stocks_minute_agg ORDER BY Datestamp DESC, Hour DESC, Minute DESC LIMIT 1000\")\n","df.show()"]},{"cell_type":"code","execution_count":null,"id":"16ceb322-bf83-47fd-83a7-c0983e53bdca","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df = spark.sql(\"SELECT min(Datestamp) as mindate, max(Datestamp) as maxdate FROM stocks_hour_agg\")\n","df.show()\n","\n","df = spark.sql(\"SELECT * FROM stocks_hour_agg ORDER BY Datestamp DESC, Hour DESC LIMIT 1000\")\n","df.show()"]},{"cell_type":"code","execution_count":null,"id":"067d791a-36ef-4183-aa29-d2e144cb4a01","metadata":{"editable":true,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[],"source":["def deleteRowsFromTable():\n","    spark.sql(\"DELETE FROM stocks_minute_agg\")\n","    spark.sql(\"DELETE FROM stocks_hour_agg\")\n","\n","# deleteRowsFromTable()"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
