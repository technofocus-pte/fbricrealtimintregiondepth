{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17df4242-a0ba-47a2-a7d9-4a3cd1fa0d8a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Lakehouse 1: Import Data\n",
    "\n",
    "This notebook will download additional historic information and merge it into the raw_stock_data table. This additional data will help provide more interesting reports and data science exploration.\n",
    "\n",
    "Before starting, the raw_stock_data table should be receiving data regularly from the Eventstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de95bc6c-b751-4253-84f9-edb05e05496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# change table name if not raw_stock_data\n",
    "targetTableName = 'raw_stock_data'\n",
    "daysToImport = 30\n",
    "\n",
    "minDateInRaw = datetime.datetime.utcnow() # will be recalculated as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb998e3-73cb-4f96-af09-f75cdedfedcd",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Download historical data\n",
    "\n",
    "The cells below will download and unzip historical data to the lakehouse unmanaged files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryData:\n",
    "    def __init__(self, file_uri, filename, year) -> None:\n",
    "        self.file_uri = file_uri\n",
    "        self.filename = filename\n",
    "        self.year = year\n",
    "\n",
    "def getDownloadInfo(year):\n",
    "    if year==2023:\n",
    "        return HistoryData(\n",
    "            'https://github.com/microsoft/fabricrealtimelab/raw/refs/heads/main/files/AbboCostStockHistory/stockhistory-2023.tgz',\n",
    "            'stockhistory-2023.tgz',\n",
    "            year)\n",
    "    elif year==2024:\n",
    "        return HistoryData(\n",
    "            'https://github.com/microsoft/fabricrealtimelab/raw/refs/heads/main/files/AbboCostStockHistory/stockhistory-2024.tgz',\n",
    "            'stockhistory-2024.tgz',\n",
    "            year)\n",
    "    elif year==2025:\n",
    "        return HistoryData(\n",
    "            'https://github.com/microsoft/fabricrealtimelab/raw/refs/heads/main/files/AbboCostStockHistory/stockhistory-2025.tgz',\n",
    "            'stockhistory-2025.tgz',\n",
    "            year)\n",
    "    elif year==2026:\n",
    "        return HistoryData(\n",
    "            'https://github.com/microsoft/fabricrealtimelab/raw/refs/heads/main/files/AbboCostStockHistory/stockhistory-2026.tgz',\n",
    "            'stockhistory-2026.tgz',\n",
    "            year)\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HistoryData:\n",
    "#     def __init__(self, file_uri, filename, year) -> None:\n",
    "#         self.file_uri = file_uri\n",
    "#         self.filename = filename\n",
    "#         self.year = year\n",
    "\n",
    "# def getDownloadInfo(year):\n",
    "#     if year==2023:\n",
    "#         return HistoryData(\n",
    "#             'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2023.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=ledWmONUdRKvcpDumZHpLPqkrTLWu%2B9GrF0gMh5QK2c%3D',\n",
    "#             'stockhistory-2023.tgz',\n",
    "#             year)\n",
    "#     elif year==2024:\n",
    "#         return HistoryData(\n",
    "#             'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2024.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=TIFg2tvEww3rdTVNOKo5ef1xTx%2Bs0XAbdEARKGhOiX8%3D',\n",
    "#             'stockhistory-2024.tgz',\n",
    "#             year)\n",
    "#     elif year==2025:\n",
    "#         return HistoryData(\n",
    "#             'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2025.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=UB4QhOmsfwhPC0rE14wRJQxeiXXutHxm%2BOVnFA3xDFQ%3D',\n",
    "#             'stockhistory-2025.tgz',\n",
    "#             year)\n",
    "#     elif year==2026:\n",
    "#         return HistoryData(\n",
    "#             'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2026.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=l4tonO4SZfuCbHrheomO0WNkuYfyTTdfdNrcfu%2Fc7dU%3D',\n",
    "#             'stockhistory-2026.tgz',\n",
    "#             year)\n",
    "#     else:\n",
    "#         return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71588929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "LAKEHOUSE_FOLDER = \"/lakehouse/default\"\n",
    "DATA_FOLDER = \"Files/stockhistory/raw\"\n",
    "\n",
    "TAR_FILE_PATH = f\"/{LAKEHOUSE_FOLDER}/{DATA_FOLDER}/tar/\"\n",
    "CSV_FILE_PATH = f\"/{LAKEHOUSE_FOLDER}/{DATA_FOLDER}/csv/\"\n",
    "\n",
    "def downloadHistoryIfNotExists():\n",
    "\n",
    "    currYear = datetime.datetime.utcnow().year\n",
    "\n",
    "    if not os.path.exists(LAKEHOUSE_FOLDER):\n",
    "        # add a lakehouse if the notebook has no default lakehouse\n",
    "        # a new notebook will not link to any lakehouse by default\n",
    "        raise FileNotFoundError(\n",
    "            \"Lakehouse not found, please add a lakehouse for the notebook.\"\n",
    "        )\n",
    "    else:\n",
    "        for year in range(currYear, currYear-2, -1):\n",
    "            fileInfo = getDownloadInfo(year)\n",
    "\n",
    "            if (fileInfo is None):\n",
    "                print(f'No file exists for {year}')\n",
    "                continue\n",
    "\n",
    "            # verify if files are already in the lakehouse, and if not, download and unzip\n",
    "            if not os.path.exists(f\"{TAR_FILE_PATH}{fileInfo.filename}\"):\n",
    "                print(f'Downloading {fileInfo.filename}')\n",
    "                os.makedirs(TAR_FILE_PATH, exist_ok=True)\n",
    "                os.system(f\"wget '{fileInfo.file_uri}' -O {TAR_FILE_PATH}{fileInfo.filename}\")\n",
    "\n",
    "                #todo: better file checking\n",
    "                os.makedirs(CSV_FILE_PATH, exist_ok=True)\n",
    "                print(f'Extracting {fileInfo.filename}')\n",
    "                os.system(f\"tar -zxvf {TAR_FILE_PATH}{fileInfo.filename} -C {CSV_FILE_PATH}\")\n",
    "            else:\n",
    "                print(f'File already exists: {fileInfo.filename}')\n",
    "\n",
    "downloadHistoryIfNotExists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f8b88",
   "metadata": {},
   "source": [
    "## Verify/Create Table, find earliest date\n",
    "\n",
    "Create the target table if it doesn't exist; find minimum date in the table to act as a cut-off point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebdef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_table_if_needed():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {targetTableName} (\n",
    "            timestamp TIMESTAMP\n",
    "            ,price DOUBLE \n",
    "            ,Symbol STRING\n",
    "            )\n",
    "        USING DELTA\n",
    "        \"\"\")\n",
    "\n",
    "create_raw_table_if_needed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442fff2f-7379-4b2f-9767-32946cd8cbbb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# find earliest date in raw_stock_data table, or use today's date if none\n",
    "\n",
    "df_min_date = spark.sql(f\"SELECT coalesce(min(to_timestamp(timestamp)),current_date()) as minDate FROM {targetTableName}\")\n",
    "minDateInRaw = df_min_date.first()[\"minDate\"]\n",
    "print(f\"Min date in raw table: {minDateInRaw}\")\n",
    "\n",
    "# import data up until about 1 hour before the current data\n",
    "historicalEndDate = (minDateInRaw + datetime.timedelta(hours=-1)).replace(microsecond=0)\n",
    "historicalBeginDate = historicalEndDate + datetime.timedelta(days=-daysToImport)\n",
    "\n",
    "# # import data before current day\n",
    "# historicalEndDate = minDateInRaw.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "# historicalBeginDate = historicalEndDate + datetime.timedelta(days=-daysToImport)\n",
    "\n",
    "print(f'Historical import begin date: {historicalBeginDate}')\n",
    "print(f'Historical import end date: {historicalEndDate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8137170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify csv files are available\n",
    "\n",
    "import time\n",
    "\n",
    "path_to_check = f'{DATA_FOLDER}/csv'\n",
    "files_found = False\n",
    "check_count = 0\n",
    "\n",
    "while (files_found == False):\n",
    "    try:\n",
    "        check_count += 1\n",
    "        files = mssparkutils.fs.ls(path_to_check)\n",
    "        if (len(files) > 0):\n",
    "            files_found = True\n",
    "        print(f'Found {len(files)} CSV folders.')\n",
    "    except Exception as e:\n",
    "        if (check_count > 10):\n",
    "            print('Unable to verify CSV files. Please restart session and verify files are downloading and extracting.')\n",
    "            raise e\n",
    "        print('Checking for files...')\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4751ad7",
   "metadata": {},
   "source": [
    "## Read the data into a dataframe\n",
    "\n",
    "Loads the data and filters out data outside the timeframe window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efbae5-d607-450a-8cc3-a96d57e7504d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# read the CSV files, {year}/{month}/{day}.csv\n",
    "\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"price\", T.DoubleType()),\n",
    "    T.StructField(\"Symbol\", T.StringType()),\n",
    "    T.StructField(\"timestamp\", T.TimestampType())\n",
    "])\n",
    "\n",
    "df_stocks = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema)\n",
    "    .load(f\"{DATA_FOLDER}/csv/*/*/*.csv\")\n",
    ")\n",
    "\n",
    "df_stocks.tail(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed41be-e310-4089-8600-a737aa17a799",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# filter stocks to between min/max dates\n",
    "\n",
    "df_stocks = df_stocks.select(\"*\").where( \\\n",
    "    f'timestamp >= \"{historicalBeginDate}\" and timestamp < \"{historicalEndDate}\"').sort(\"timestamp\")\n",
    "\n",
    "df_stocks.tail(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc201d21",
   "metadata": {},
   "source": [
    "## Write to Delta table\n",
    "\n",
    "Append is the fastest method, but merge can be used if looking to update existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370e070-f9c2-47e9-a7e3-d8be4090eda0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    " # a merge offers more flexibility to update the table, but risks a concurrency error\n",
    " # because the table is being updated by the eventstream\n",
    "\n",
    " from delta.tables import *\n",
    "\n",
    " def importIntoRaw(df):\n",
    "\n",
    "   raw_table = DeltaTable.forName(spark, targetTableName)\n",
    "\n",
    "   raw_table.alias('raw') \\\n",
    "     .merge(\n",
    "       df.alias('history'),\n",
    "       'raw.timestamp = history.timestamp and raw.symbol = history.symbol'\n",
    "     ) \\\n",
    "     .whenMatchedUpdate(set =\n",
    "         {\n",
    "           \"price\": \"history.price\"\n",
    "         }\n",
    "     ) \\\n",
    "     .whenNotMatchedInsert(values =\n",
    "       {\n",
    "           \"symbol\": \"history.symbol\"\n",
    "           ,\"price\": \"history.price\"\n",
    "           ,\"timestamp\": \"history.timestamp\"\n",
    "       }\n",
    "     ) \\\n",
    "     .execute()\n",
    "\n",
    " Retries = 3\n",
    " IsSuccess = False\n",
    "\n",
    " for i in range(Retries):\n",
    "   try:\n",
    "     importIntoRaw(df_stocks)\n",
    "     IsSuccess = True\n",
    "     print(f\"Completed merge\")\n",
    "     break\n",
    "   except delta.exceptions.ConcurrentAppendException as e:\n",
    "     print(f\"Concurrency error - please wait and try again: {e}\")\n",
    "     time.sleep(1)\n",
    "     continue\n",
    "\n",
    " if not IsSuccess:\n",
    "   msg = f\"Failed to merge after {Retries} retries\"\n",
    "   raise SystemExit(msg)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python"
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "trident": {
   "lakehouse": {}
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
